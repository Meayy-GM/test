import streamlit as st
import pandas as pd
from transformers import pipeline
import torch
import re

st.set_page_config(page_title="Grammar Checker", layout="wide")
st.title("ğŸ” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‹±æ–‡æ³•ãƒã‚§ãƒƒã‚«ãƒ¼")

st.markdown("""
è‹±èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ãƒŸã‚¹ã‚’æ¤œå‡ºã—ã¦ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚  
Hugging Face Transformers ã‚’ä½¿ç”¨ã—ã¦åˆ†æã—ã¦ã„ã¾ã™ã€‚
""")

user_text = st.text_area(
    "è‹±æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", 
    height=200, 
    placeholder="ã“ã“ã«è‹±æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã™ãã«ãƒã‚§ãƒƒã‚¯çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
)

# Transformersãƒ¢ãƒ‡ãƒ«ã‚’ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def load_grammar_model():
    try:
        # æ–‡æ³•ä¿®æ­£ç”¨ã®ãƒ¢ãƒ‡ãƒ«
        corrector = pipeline(
            "text2text-generation", 
            model="vennify/t5-base-grammar-correction",
            device=0 if torch.cuda.is_available() else -1
        )
        return corrector
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def split_text_into_chunks(text, max_length=400):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²"""
    if len(text) <= max_length:
        return [text]
    
    # æ–‡ã§åˆ†å‰²
    sentences = re.split(r'[.!?]+\s+', text)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk + sentence) <= max_length:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def find_differences(original, corrected):
    """åŸæ–‡ã¨ä¿®æ­£æ–‡ã®é•ã„ã‚’è¦‹ã¤ã‘ã‚‹"""
    original_words = original.split()
    corrected_words = corrected.split()
    
    differences = []
    i, j = 0, 0
    
    while i < len(original_words) and j < len(corrected_words):
        if original_words[i] != corrected_words[j]:
            # é•ã„ã‚’ç™ºè¦‹
            original_phrase = original_words[i]
            corrected_phrase = corrected_words[j]
            
            # è¤‡æ•°å˜èªã®å¤‰æ›´ã‚’ã‚­ãƒ£ãƒƒãƒ
            k = 1
            while (i + k < len(original_words) and 
                   j + k < len(corrected_words) and
                   ' '.join(original_words[i:i+k+1]) != ' '.join(corrected_words[j:j+k+1])):
                k += 1
            
            if k > 1:
                original_phrase = ' '.join(original_words[i:i+k])
                corrected_phrase = ' '.join(corrected_words[j:j+k])
                i += k
                j += k
            else:
                i += 1
                j += 1
            
            differences.append({
                "ã‚¨ãƒ©ãƒ¼ç®‡æ‰€": original_phrase,
                "ç†ç”±": "æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ä¿®æ­£",
                "ææ¡ˆ": corrected_phrase
            })
        else:
            i += 1
            j += 1
    
    return differences

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
    corrector = load_grammar_model()

if user_text.strip() and corrector:
    with st.spinner("æ–‡æ³•ãƒã‚§ãƒƒã‚¯ä¸­..."):
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
            chunks = split_text_into_chunks(user_text)
            corrected_chunks = []
            all_differences = []
            
            for chunk in chunks:
                # æ–‡æ³•ä¿®æ­£ã‚’å®Ÿè¡Œ
                result = corrector(f"grammar: {chunk}", max_length=len(chunk) + 100)
                corrected_chunk = result[0]['generated_text']
                corrected_chunks.append(corrected_chunk)
                
                # é•ã„ã‚’æ¤œå‡º
                differences = find_differences(chunk, corrected_chunk)
                all_differences.extend(differences)
            
            corrected_text = ' '.join(corrected_chunks)
            
            if not all_differences:
                st.success("âœ… æ–‡æ³•ãƒŸã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.warning(f"ğŸš¨ {len(all_differences)} ä»¶ã®æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ãƒŸã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                
                df = pd.DataFrame(all_differences)
                st.dataframe(df, use_container_width=True)

            # ä¿®æ­£æ¸ˆã¿æ–‡ç« ã‚’æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤º
            if corrected_text.strip() != user_text.strip():
                with st.expander("âœï¸ ä¿®æ­£å¾Œã®æ–‡ç« ã‚’è¡¨ç¤º"):
                    st.code(corrected_text, language='markdown')
        
        except Exception as e:
            st.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

elif user_text.strip() and not corrector:
    st.error("ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
else:
    st.info("â¬… ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è‹±èªã®æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã‚’è¡¨ç¤º
with st.expander("ğŸ“‹ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª"):
    st.code("""
pip install transformers torch pandas streamlit
    """, language='bash')