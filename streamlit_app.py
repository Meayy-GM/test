import streamlit as st
import pandas as pd
from gingerit.gingerit import GingerIt
import re

st.set_page_config(page_title="Grammar Checker", layout="wide")
st.title("ğŸ” ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‹±æ–‡æ³•ãƒã‚§ãƒƒã‚«ãƒ¼")

st.markdown("""
è‹±èªã®æ–‡ç« ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ãƒŸã‚¹ã‚’æ¤œå‡ºã—ã¦ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚  
Gingerit ã‚’ä½¿ç”¨ã—ã¦åˆ†æã—ã¦ã„ã¾ã™ã€‚
""")

user_text = st.text_area(
    "è‹±æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", 
    height=200, 
    placeholder="ã“ã“ã«è‹±æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã™ãã«ãƒã‚§ãƒƒã‚¯çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
)

# Gingeritã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä¸€åº¦ã ã‘ä½œæˆ
@st.cache_resource
def get_parser():
    return GingerIt()

parser = get_parser()

def split_long_text(text, max_length=600):
    """é•·ã„æ–‡ç« ã‚’çŸ­ã„ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    if len(text) <= max_length:
        return [text]
    
    # æ–‡ã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
    sentences = re.split(r'[.!?]+\s+', text)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk + sentence) <= max_length:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

if user_text.strip():
    try:
        # é•·æ–‡ã®å ´åˆã¯åˆ†å‰²ã—ã¦å‡¦ç†
        chunks = split_long_text(user_text)
        all_issues = []
        corrected_chunks = []
        
        offset = 0
        for chunk in chunks:
            result = parser.parse(chunk)
            corrected_chunks.append(result['result'])
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
            if 'corrections' in result:
                for correction in result['corrections']:
                    all_issues.append({
                        "ã‚¨ãƒ©ãƒ¼ç®‡æ‰€": correction.get('text', ''),
                        "ç†ç”±": correction.get('definition', 'æ–‡æ³•ã‚¨ãƒ©ãƒ¼'),
                        "ææ¡ˆ": correction.get('correct', 'ï¼ˆææ¡ˆãªã—ï¼‰')
                    })
            
            offset += len(chunk)
        
        corrected_text = ' '.join(corrected_chunks)
        
        if not all_issues:
            # å…ƒã®æ–‡ç« ã¨ä¿®æ­£å¾Œã‚’æ¯”è¼ƒã—ã¦é•ã„ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if user_text.strip() != corrected_text.strip():
                st.warning("ğŸš¨ æ–‡ç« ãŒä¿®æ­£ã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ãªåˆ†æçµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                all_issues = [{
                    "ã‚¨ãƒ©ãƒ¼ç®‡æ‰€": "å…¨ä½“",
                    "ç†ç”±": "æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ã®æ”¹å–„ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ",
                    "ææ¡ˆ": "ä¿®æ­£å¾Œã®æ–‡ç« ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                }]
            else:
                st.success("âœ… æ–‡æ³•ãƒŸã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.warning(f"ğŸš¨ {len(all_issues)} ä»¶ã®æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ãƒŸã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

        if all_issues:
            df = pd.DataFrame(all_issues)
            st.dataframe(df, use_container_width=True)

        # ä¿®æ­£æ¸ˆã¿æ–‡ç« ã‚’æŠ˜ã‚ŠãŸãŸã¿è¡¨ç¤º
        if corrected_text != user_text:
            with st.expander("âœï¸ ä¿®æ­£å¾Œã®æ–‡ç« ã‚’è¡¨ç¤º"):
                st.code(corrected_text, language='markdown')

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.info("é•·ã„æ–‡ç« ã®å ´åˆã€è¤‡æ•°å›ã«åˆ†ã‘ã¦å…¥åŠ›ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")

else:
    st.info("â¬… ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è‹±èªã®æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")